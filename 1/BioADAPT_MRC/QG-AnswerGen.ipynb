{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20f6c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import re \n",
    "import numpy as np \n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, nlp):\n",
    "        self.vocab = nlp.vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.make_doc = WhitespaceTokenizer(nlp)\n",
    "\n",
    "def pos_regex_matches(doc, pattern):\n",
    "    \"\"\"\n",
    "    Extract sequences of consecutive tokens from a spacy-parsed doc whose\n",
    "    part-of-speech tags match the specified regex pattern.\n",
    "\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc`` or ``spacy.Span``)\n",
    "        pattern (str): Pattern of consecutive POS tags whose corresponding words\n",
    "            are to be extracted, inspired by the regex patterns used in NLTK's\n",
    "            `nltk.chunk.regexp`. Tags are uppercase, from the universal tag set;\n",
    "            delimited by < and >, which are basically converted to parentheses\n",
    "            with spaces as needed to correctly extract matching word sequences;\n",
    "            white space in the input doesn't matter.\n",
    "\n",
    "            Examples (see ``constants.POS_REGEX_PATTERNS``):\n",
    "\n",
    "            * noun phrase: r'<DET>? (<NOUN>+ <ADP|CONJ>)* <NOUN>+'\n",
    "            * compound nouns: r'<NOUN>+'\n",
    "            * verb phrase: r'<VERB>?<ADV>*<VERB>+'\n",
    "            * prepositional phrase: r'<PREP> <DET>? (<NOUN>+<ADP>)* <NOUN>+'\n",
    "\n",
    "    Yields:\n",
    "        ``spacy.Span``: the next span of consecutive tokens from ``doc`` whose\n",
    "            parts-of-speech match ``pattern``, in order of apperance\n",
    "    \"\"\"\n",
    "    # standardize and transform the regular expression pattern...\n",
    "    pattern = re.sub(r'\\s', '', pattern)\n",
    "    pattern = re.sub(r'<([A-Z]+)\\|([A-Z]+)>', r'( (\\1|\\2))', pattern)\n",
    "    pattern = re.sub(r'<([A-Z]+)\\|([A-Z]+)\\|([A-Z]+)>', r'( (\\1|\\2|\\3))', pattern)\n",
    "    pattern = re.sub(r'<([A-Z]+)\\|([A-Z]+)\\|([A-Z]+)\\|([A-Z]+)>', r'( (\\1|\\2|\\3|\\4))', pattern)\n",
    "    pattern = re.sub(r'<([A-Z]+)>', r'( \\1)', pattern)\n",
    "\n",
    "\n",
    "    tags = ' ' + ' '.join(tok.pos_ for tok in doc)\n",
    "    toks = list(map(lambda t: t.pos_, doc))\n",
    "\n",
    "    for m in re.finditer(pattern, tags):\n",
    "        start_index = tags[0:m.start()].count(' ')\n",
    "        end_index = tags[0:m.end()].count(' ')\n",
    "        #yield (start_index, end_index)\n",
    "        yield start_index, end_index, doc[tags[0:m.start()].count(' '):tags[0:m.end()].count(' ')]\n",
    "\n",
    "def extract_NER(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if \"CNN\" not in ent.text:\n",
    "            ent_mappings[ent.label_].append(ent)\n",
    "\n",
    "def sample(arr, num_samples):\n",
    "    num_samples = np.min([len(arr), num_samples])\n",
    "    entities = np.random.permutation(arr)[0:num_samples]\n",
    "    return entities\n",
    "\n",
    "def extract_phrases(text, num_samples=2):\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    doc = nlp(text)\n",
    "#     for tok in doc:\n",
    "#       print(tok, tok.pos_)\n",
    "\n",
    "    noun_phrase_pattern = '<DET>? (<NOUN|PROPN>+ <ADP|CONJ|CCONJ|PUNCT>*)* <NOUN|PROPN>+'\n",
    "    verb_phrase_pattern = '<VERB>?<ADV>*<VERB>+'\n",
    "    prepositional_phrase_pattern = '<PREP> <DET>? (<NOUN>+<ADP>)* <NOUN>+'\n",
    "\n",
    "    noun_phrases = list(pos_regex_matches(doc, noun_phrase_pattern))\n",
    "    verb_phrases = list(pos_regex_matches(doc, verb_phrase_pattern))\n",
    "    noun_chunks = np.array(list(doc.noun_chunks))\n",
    "\n",
    "    if len(noun_chunks) > 1:\n",
    "        lengths = list(map(lambda l: len(l), noun_chunks))\n",
    "        sorted_indices = np.argsort(lengths)\n",
    "        top_sorted_indices = np.min([len(sorted_indices), 3])\n",
    "        top_sorted_indices = sorted_indices[-top_sorted_indices:]\n",
    "        top_chunks = noun_chunks[top_sorted_indices]\n",
    "        \n",
    "        for i in range(0, len(top_chunks)):\n",
    "            cur_chunk = top_chunks[i]\n",
    "            if type(cur_chunk) == type(np.array([])):\n",
    "                print(\"Invalid chunk given\")\n",
    "                continue\n",
    "            cur_start = cur_chunk.start \n",
    "            cur_end = cur_chunk.end \n",
    "            start_indices.append(cur_start)\n",
    "            end_indices.append(cur_end)\n",
    "\n",
    "            #print(cur_chunk)\n",
    "\n",
    "    ent_mappings = defaultdict(list)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if \"CNN\" not in ent.text:\n",
    "            ent_mappings[ent.label_].append(ent)\n",
    "\n",
    "#     #print(\"Printing entities\")\n",
    "#     #print(ent_mappings)\n",
    "\n",
    "#     filtered_verb_phrases = list(filter(lambda vp: len(vp[2]) > 1, verb_phrases))\n",
    "#     filtered_noun_phrases = list(filter(lambda np: len(np[2]) > 1, noun_phrases))\n",
    "\n",
    "    for k in ent_mappings:\n",
    "        entities = ent_mappings[k]\n",
    "\n",
    "        if len(entities) == 1: \n",
    "            random_entities = [entities]\n",
    "        else:\n",
    "            random_entities = sample(entities, num_samples)\n",
    "\n",
    "        for i in range(0, len(random_entities)):\n",
    "            ent = entities[i]\n",
    "            start_indices.append(ent.start)\n",
    "            end_indices.append(ent.end)\n",
    "\n",
    "    random_noun_phrases = sample(noun_phrases, num_samples)\n",
    "    random_verb_phrases = sample(verb_phrases, num_samples)\n",
    "    \n",
    "    for phrase in random_noun_phrases:\n",
    "        start_indices.append(phrase[0])\n",
    "        end_indices.append(phrase[1])\n",
    "        #print(doc[phrase[0]:phrase[1]])\n",
    "\n",
    "    for phrase in random_verb_phrases:\n",
    "        start_indices.append(phrase[0])\n",
    "        end_indices.append(phrase[1])\n",
    "\n",
    "    return start_indices, end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "503782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d88fa5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "Psoriasiform dermatitis in a case of newly diagnosed locally advanced pyriform sinus tumour: Bazex syndrome revisited. Acrokeratosis paraneoplastica of Bazex is a rare but important paraneoplastic dermatosis, usually manifesting as psoriasiform rashes over the acral sites. It often precedes diagnosis of the associated malignancy, usually that of upper aerodigestive tract squamous cell carcinoma. We present the case of a patient with a newly diagnosed pyriform sinus tumour and associated acrokeratosis paraneoplastica. To the best of our knowledge, this is the first reported case in the local literature.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a186174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/kdinxidk03/opt/NFS/75y/anaconda3/envs/QA/lib/python3.7/site-packages/ipykernel_launcher.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "st, en = extract_phrases(t, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d117094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([79, 21, 61, 19, 86, 80, 25, 35, 37, 62],\n",
       " [83, 27, 67, 20, 87, 81, 27, 36, 39, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22ac2e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a newly diagnosed pyriform sinus tumour"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(t)\n",
    "\n",
    "doc[61:67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m tests.language_model_trainer_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
