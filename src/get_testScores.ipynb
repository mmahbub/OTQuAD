{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 3e-5\n",
    "# batch size = 24\n",
    "# fine-tuning epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbbca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python3 finetune_model.py  \\\n",
    "#     --model_type bluebert   \\\n",
    "#     --orig_model_name_or_path bluebert  \\\n",
    "#     --model_name_or_path bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 \\\n",
    "#     --output_dir /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/ \\\n",
    "#     --output_model_dir /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-squad/ \\\n",
    "#     --data_dir /net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/squad/  \\\n",
    "#     --train_file train_squad.json  \\\n",
    "#     --dataset_name squad  \\\n",
    "#     --do_train \\\n",
    "#     --max_seq_length 384   \\\n",
    "#     --doc_stride 128  \\\n",
    "#     --per_gpu_train_batch_size 24 \\\n",
    "#     --num_train_epochs 3.0 \\\n",
    "#     --learning_rate  3e-5  \\\n",
    "#     --overwrite_output_dir  \\\n",
    "#     --overwrite_output_model_dir \\\n",
    "# #     --overwrite_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229e500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a2fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python3 evaluate.py  \\\n",
    "#     --model_type biolinkbert   \\\n",
    "#     --orig_model_name_or_path biolinkbert  \\\n",
    "#     --model_name_or_path /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-emrqa/ \\\n",
    "#     --output_dir /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/ \\\n",
    "#     --output_model_dir None \\\n",
    "#     --data_dir /net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/emrqa/  \\\n",
    "#     --predict_file test_emrqa.json  \\\n",
    "#     --dataset_name emrqa  \\\n",
    "#     --do_evaluate \\\n",
    "#     --max_seq_length 384   \\\n",
    "#     --doc_stride 128  \\\n",
    "#     --do_lower_case \\\n",
    "#     --per_gpu_eval_batch_size 24 \\\n",
    "#     --overwrite_output_dir  \\\n",
    "#     --overwrite_output_model_dir \\\n",
    "# #     --overwrite_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b725b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8baad21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python3 run_sequential.py\n",
    "# !python3 run_baselines_only_source.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d0d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/08/2022 20:38:38 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:38:38,785 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:38:38,786 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:38:38,787 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:38:38,787 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:38,787 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:38,787 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:38,788 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:38,788 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:38,788 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:38:38,877 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:38:41,118 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:38:41,118 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:38:43 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/', dataset_name='otquad_fold_1', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_otquad_fold_1.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "09/08/2022 20:38:43 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/ for evaluation\n",
      "09/08/2022 20:38:43 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/']\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:38:43,540 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:38:43,541 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:38:43,541 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:38:44,499 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:38:44,500 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:38:44 - INFO - __main__ - Creating features from dataset file at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/cached_dev_biolinkbert_384_test_otquad_fold_1\n",
      "100%|█████████████████████████████████████████| 85/85 [00:00<00:00, 3270.67it/s]\n",
      "convert squad examples to features: 100%|██████| 85/85 [00:00<00:00, 149.29it/s]\n",
      "add example index and unique id: 100%|██████| 85/85 [00:00<00:00, 212591.44it/s]\n",
      "09/08/2022 20:38:45 - INFO - __main__ - Saving features into cached file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/cached_dev_biolinkbert_384_test_otquad_fold_1\n",
      "09/08/2022 20:38:45 - INFO - __main__ - ***** Running evaluation otquad_fold_1 *****\n",
      "09/08/2022 20:38:45 - INFO - __main__ -   Num examples = 85\n",
      "09/08/2022 20:38:45 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|█████████████████████████████████| 4/4 [00:00<00:00,  4.28it/s]\n",
      "09/08/2022 20:38:46 - INFO - __main__ -   Evaluation done in total 0.935804 secs (0.011009 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-09-08 20:38:46,366 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/predictions_otquad_fold_1.json\n",
      "[INFO|squad_metrics.py:403] 2022-09-08 20:38:46,366 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold1/nbest_predictions_otquad_fold_1.json\n",
      "09/08/2022 20:38:46 - INFO - __main__ - Results: {'exact': 65.88235294117646, 'f1': 86.2652529256011, 'total': 85, 'HasAns_exact': 65.88235294117646, 'HasAns_f1': 86.2652529256011, 'HasAns_total': 85, 'best_exact': 65.88235294117646, 'best_exact_thresh': 0.0, 'best_f1': 86.2652529256011, 'best_f1_thresh': 0.0}\n",
      "09/08/2022 20:38:49 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:38:49,704 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:38:49,704 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:38:49,705 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:38:49,705 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:49,706 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:49,706 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:49,706 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:49,706 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:38:49,706 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:38:49,782 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:38:51,965 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:38:51,965 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:38:54 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/', dataset_name='otquad_fold_2', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_otquad_fold_2.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "09/08/2022 20:38:54 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/ for evaluation\n",
      "09/08/2022 20:38:54 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/']\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:38:54,470 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:38:54,471 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:38:54,471 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:38:55,447 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:38:55,447 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:38:55 - INFO - __main__ - Creating features from dataset file at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/cached_dev_biolinkbert_384_test_otquad_fold_2\n",
      "100%|███████████████████████████████████████| 158/158 [00:00<00:00, 2663.60it/s]\n",
      "convert squad examples to features: 100%|████| 158/158 [00:01<00:00, 127.45it/s]\n",
      "add example index and unique id: 100%|████| 158/158 [00:00<00:00, 274182.88it/s]\n",
      "09/08/2022 20:38:57 - INFO - __main__ - Saving features into cached file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/cached_dev_biolinkbert_384_test_otquad_fold_2\n",
      "09/08/2022 20:38:57 - INFO - __main__ - ***** Running evaluation otquad_fold_2 *****\n",
      "09/08/2022 20:38:57 - INFO - __main__ -   Num examples = 160\n",
      "09/08/2022 20:38:57 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|█████████████████████████████████| 7/7 [00:01<00:00,  4.58it/s]\n",
      "09/08/2022 20:38:58 - INFO - __main__ -   Evaluation done in total 1.530040 secs (0.009563 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-09-08 20:38:58,791 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/predictions_otquad_fold_2.json\n",
      "[INFO|squad_metrics.py:403] 2022-09-08 20:38:58,791 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/nbest_predictions_otquad_fold_2.json\n",
      "09/08/2022 20:38:59 - INFO - __main__ - Results: {'exact': 66.45569620253164, 'f1': 86.82604438674124, 'total': 158, 'HasAns_exact': 66.45569620253164, 'HasAns_f1': 86.82604438674124, 'HasAns_total': 158, 'best_exact': 66.45569620253164, 'best_exact_thresh': 0.0, 'best_f1': 86.82604438674124, 'best_f1_thresh': 0.0}\n",
      "09/08/2022 20:39:02 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:02,610 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:02,610 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:02,611 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:02,611 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/tokenizer.json. We won't load it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:02,611 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:02,611 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:02,612 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:02,612 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:02,612 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:02,687 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:04,974 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:04,974 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:39:07 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/', dataset_name='otquad_fold_3', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_otquad_fold_3.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "09/08/2022 20:39:07 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/ for evaluation\n",
      "09/08/2022 20:39:07 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/']\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:07,428 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:07,428 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:07,429 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:08,493 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:08,493 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:39:08 - INFO - __main__ - Creating features from dataset file at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/cached_dev_biolinkbert_384_test_otquad_fold_3\n",
      "100%|███████████████████████████████████████| 241/241 [00:00<00:00, 2299.05it/s]\n",
      "convert squad examples to features: 100%|█████| 241/241 [00:02<00:00, 99.57it/s]\n",
      "add example index and unique id: 100%|████| 241/241 [00:00<00:00, 317072.54it/s]\n",
      "09/08/2022 20:39:11 - INFO - __main__ - Saving features into cached file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/cached_dev_biolinkbert_384_test_otquad_fold_3\n",
      "09/08/2022 20:39:11 - INFO - __main__ - ***** Running evaluation otquad_fold_3 *****\n",
      "09/08/2022 20:39:11 - INFO - __main__ -   Num examples = 270\n",
      "09/08/2022 20:39:11 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 12/12 [00:02<00:00,  5.27it/s]\n",
      "09/08/2022 20:39:13 - INFO - __main__ -   Evaluation done in total 2.276735 secs (0.008432 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-09-08 20:39:13,927 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/predictions_otquad_fold_3.json\n",
      "[INFO|squad_metrics.py:403] 2022-09-08 20:39:13,927 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold3/nbest_predictions_otquad_fold_3.json\n",
      "09/08/2022 20:39:15 - INFO - __main__ - Results: {'exact': 68.04979253112033, 'f1': 83.73472899524796, 'total': 241, 'HasAns_exact': 68.04979253112033, 'HasAns_f1': 83.73472899524796, 'HasAns_total': 241, 'best_exact': 68.04979253112033, 'best_exact_thresh': 0.0, 'best_f1': 83.73472899524796, 'best_f1_thresh': 0.0}\n",
      "09/08/2022 20:39:18 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:18,471 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:18,472 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:18,473 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:18,473 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:18,473 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:18,473 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:18,473 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:18,473 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:18,473 >> loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:18,562 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:20,799 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:20,800 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:39:23 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/', dataset_name='otquad_fold_4', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_otquad_fold_4.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "09/08/2022 20:39:23 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/ for evaluation\n",
      "09/08/2022 20:39:23 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/']\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:23,159 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:23,159 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:23,160 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:24,093 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:24,093 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:39:24 - INFO - __main__ - Creating features from dataset file at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/cached_dev_biolinkbert_384_test_otquad_fold_4\n",
      "100%|███████████████████████████████████████| 228/228 [00:00<00:00, 2765.06it/s]\n",
      "convert squad examples to features: 100%|████| 228/228 [00:01<00:00, 125.51it/s]\n",
      "add example index and unique id: 100%|████| 228/228 [00:00<00:00, 338753.56it/s]\n",
      "09/08/2022 20:39:26 - INFO - __main__ - Saving features into cached file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/cached_dev_biolinkbert_384_test_otquad_fold_4\n",
      "09/08/2022 20:39:26 - INFO - __main__ - ***** Running evaluation otquad_fold_4 *****\n",
      "09/08/2022 20:39:26 - INFO - __main__ -   Num examples = 228\n",
      "09/08/2022 20:39:26 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 10/10 [00:01<00:00,  5.10it/s]\n",
      "09/08/2022 20:39:28 - INFO - __main__ -   Evaluation done in total 1.961193 secs (0.008602 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-09-08 20:39:28,539 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/predictions_otquad_fold_4.json\n",
      "[INFO|squad_metrics.py:403] 2022-09-08 20:39:28,539 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold4/nbest_predictions_otquad_fold_4.json\n",
      "09/08/2022 20:39:30 - INFO - __main__ - Results: {'exact': 72.36842105263158, 'f1': 88.43667688149412, 'total': 228, 'HasAns_exact': 72.36842105263158, 'HasAns_f1': 88.43667688149412, 'HasAns_total': 228, 'best_exact': 72.36842105263158, 'best_exact_thresh': 0.0, 'best_f1': 88.43667688149412, 'best_f1_thresh': 0.0}\n",
      "09/08/2022 20:39:33 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:33,052 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:33,053 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:33,054 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-09-08 20:39:33,054 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:33,054 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:33,054 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:33,054 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:33,054 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-09-08 20:39:33,054 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:33,130 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:35,487 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:35,487 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/08/2022 20:39:38 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/', dataset_name='otquad_fold_5', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_otquad_fold_5.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "09/08/2022 20:39:38 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/ for evaluation\n",
      "09/08/2022 20:39:38 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/']\n",
      "[INFO|configuration_utils.py:528] 2022-09-08 20:39:38,099 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-09-08 20:39:38,100 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-09-08 20:39:38,100 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-09-08 20:39:39,059 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-09-08 20:39:39,060 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "09/08/2022 20:39:39 - INFO - __main__ - Creating features from dataset file at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/cached_dev_biolinkbert_384_test_otquad_fold_5\n",
      "100%|███████████████████████████████████████| 385/385 [00:00<00:00, 1715.40it/s]\n",
      "convert squad examples to features: 100%|█████| 385/385 [00:05<00:00, 66.92it/s]\n",
      "add example index and unique id: 100%|████| 385/385 [00:00<00:00, 322574.32it/s]\n",
      "09/08/2022 20:39:45 - INFO - __main__ - Saving features into cached file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/cached_dev_biolinkbert_384_test_otquad_fold_5\n",
      "09/08/2022 20:39:46 - INFO - __main__ - ***** Running evaluation otquad_fold_5 *****\n",
      "09/08/2022 20:39:46 - INFO - __main__ -   Num examples = 559\n",
      "09/08/2022 20:39:46 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 24/24 [00:04<00:00,  5.47it/s]\n",
      "09/08/2022 20:39:50 - INFO - __main__ -   Evaluation done in total 4.387795 secs (0.007849 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-09-08 20:39:50,439 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/predictions_otquad_fold_5.json\n",
      "[INFO|squad_metrics.py:403] 2022-09-08 20:39:50,439 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold5/nbest_predictions_otquad_fold_5.json\n",
      "09/08/2022 20:39:52 - INFO - __main__ - Results: {'exact': 71.42857142857143, 'f1': 87.79145657537555, 'total': 385, 'HasAns_exact': 71.42857142857143, 'HasAns_f1': 87.79145657537555, 'HasAns_total': 385, 'best_exact': 71.42857142857143, 'best_exact_thresh': 0.0, 'best_f1': 87.79145657537555, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## unsup\n",
    "# /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squad/nbest_predictions_otquad.json\n",
    "# /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squad/nbest_predictions_squad.json\n",
    "\n",
    "# ## sup\n",
    "# /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-squadANDotquad-fold2/nbest_predictions_otquad_fold_{numFold}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586aaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f124245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9751cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 evaluate.py  \\\n",
    "    --model_type scibert   \\\n",
    "    --orig_model_name_or_path scibert  \\\n",
    "    --model_name_or_path /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-emrqa/ \\\n",
    "    --output_dir /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/ \\\n",
    "    --output_model_dir None \\\n",
    "    --data_dir /net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/emrqa/  \\\n",
    "    --predict_file test_emrqa.json  \\\n",
    "    --dataset_name emrqa  \\\n",
    "    --do_evaluate \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128  \\\n",
    "    --do_lower_case \\\n",
    "    --per_gpu_eval_batch_size 24 \\\n",
    "    --overwrite_output_dir  \\\n",
    "    --overwrite_output_model_dir \\\n",
    "#     --overwrite_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb49790",
   "metadata": {},
   "outputs": [],
   "source": [
    "biolinkbert - uncased\n",
    "pubmedbert - uncased\n",
    "bioelectra - cased\n",
    "biobert - cased\n",
    "scibert - uncased\n",
    "clinicalbert - cased\n",
    "roberta - cased\n",
    "bluebert - cased\n",
    "bert - uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce1e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 evaluate.py  \\\n",
    "    --model_type scibert   \\\n",
    "    --orig_model_name_or_path scibert  \\\n",
    "    --model_name_or_path /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-emrqa/ \\\n",
    "    --output_dir /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/ \\\n",
    "    --output_model_dir None \\\n",
    "    --data_dir /net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/otquad/  \\\n",
    "    --predict_file otquad.json  \\\n",
    "    --dataset_name otquad  \\\n",
    "    --do_evaluate \\\n",
    "    --max_seq_length 384   \\\n",
    "    --doc_stride 128  \\\n",
    "    --do_lower_case \\\n",
    "    --per_gpu_eval_batch_size 24 \\\n",
    "    --overwrite_output_dir  \\\n",
    "    --overwrite_output_model_dir \\\n",
    "    --overwrite_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44b6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
