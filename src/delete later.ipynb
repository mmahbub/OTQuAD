{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d663c846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/11/2022 17:43:28 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:43:28,030 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:43:28,030 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:43:28,031 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:43:28,031 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:43:28,032 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:43:28,032 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:43:28,032 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:43:28,032 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:43:28,032 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:43:28,110 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:43:29,112 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:43:29,112 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:43:35 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/', model_type='biolinkbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biolinkbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:43:35 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/ for evaluation\n",
      "08/11/2022 17:43:35 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:43:35,897 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:43:35,897 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"michiyasunaga/BioLinkBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28895\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:43:35,898 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:43:36,895 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:43:36,895 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biolinkbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:43:37 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_biolinkbert_384_test_bioasq\n",
      "08/11/2022 17:43:37 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:43:37 - INFO - __main__ -   Num examples = 937\n",
      "08/11/2022 17:43:37 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:24<00:00,  1.65it/s]\n",
      "08/11/2022 17:44:02 - INFO - __main__ -   Evaluation done in total 24.192902 secs (0.025820 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:44:02,076 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:44:02,076 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:44:05 - INFO - __main__ - Results: {'exact': 64.24759871931697, 'f1': 74.40509445794461, 'total': 937, 'HasAns_exact': 64.24759871931697, 'HasAns_f1': 74.40509445794461, 'HasAns_total': 937, 'best_exact': 64.24759871931697, 'best_exact_thresh': 0.0, 'best_f1': 74.40509445794461, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:44:08 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:44:08,725 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:44:08,726 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:44:08,727 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:44:08,727 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:08,727 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:08,727 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:08,727 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:08,727 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:08,727 >> loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:44:08,820 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:44:09,897 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:44:09,897 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:44:16 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/', model_type='pubmedbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='pubmedbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:44:16 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/ for evaluation\n",
      "08/11/2022 17:44:16 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:44:16,476 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:44:16,477 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:44:16,477 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:44:17,473 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:44:17,473 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/pubmedbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:44:18 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_pubmedbert_384_test_bioasq\n",
      "08/11/2022 17:44:18 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:44:18 - INFO - __main__ -   Num examples = 937\n",
      "08/11/2022 17:44:18 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:24<00:00,  1.64it/s]\n",
      "08/11/2022 17:44:42 - INFO - __main__ -   Evaluation done in total 24.443506 secs (0.026087 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:44:42,860 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:44:42,860 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:44:46 - INFO - __main__ - Results: {'exact': 59.65848452508004, 'f1': 72.68738623140001, 'total': 937, 'HasAns_exact': 59.65848452508004, 'HasAns_f1': 72.68738623140001, 'HasAns_total': 937, 'best_exact': 59.65848452508004, 'best_exact_thresh': 0.0, 'best_f1': 72.68738623140001, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:44:49 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:44:49,983 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:44:49,985 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:44:49,987 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:44:49,988 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:49,988 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:49,988 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:49,988 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:49,988 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:44:49,988 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:44:50,174 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:44:51,372 >> All model checkpoint weights were used when initializing ElectraForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:44:51,373 >> All the weights of ElectraForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:44:58 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/', model_type='bioelectra', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='bioelectra', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:44:58 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/ for evaluation\n",
      "08/11/2022 17:44:58 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:44:58,854 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:44:58,854 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:44:58,855 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:44:59,996 >> All model checkpoint weights were used when initializing ElectraForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:44:59,997 >> All the weights of ElectraForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bioelectra-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:45:00 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_bioelectra_384_test_bioasq\n",
      "08/11/2022 17:45:00 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:45:00 - INFO - __main__ -   Num examples = 937\n",
      "08/11/2022 17:45:00 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:20<00:00,  1.97it/s]\n",
      "08/11/2022 17:45:21 - INFO - __main__ -   Evaluation done in total 20.343748 secs (0.021712 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:45:21,285 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:45:21,285 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:45:26 - INFO - __main__ - Results: {'exact': 52.40128068303095, 'f1': 64.7758459640462, 'total': 937, 'HasAns_exact': 52.40128068303095, 'HasAns_f1': 64.7758459640462, 'HasAns_total': 937, 'best_exact': 52.40128068303095, 'best_exact_thresh': 0.0, 'best_f1': 64.7758459640462, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:45:30 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:45:30,107 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:45:30,108 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:45:30,109 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:45:30,109 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:45:30,110 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:45:30,110 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:45:30,110 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:45:30,110 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:45:30,110 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:45:30,203 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:45:31,191 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:45:31,192 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:45:37 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/', model_type='biobert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='biobert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:45:37 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/ for evaluation\n",
      "08/11/2022 17:45:37 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:45:37,151 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:45:37,151 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:45:37,152 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:45:38,162 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:45:38,163 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/biobert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:45:38 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_biobert_384_test_bioasq\n",
      "08/11/2022 17:45:38 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:45:38 - INFO - __main__ -   Num examples = 940\n",
      "08/11/2022 17:45:38 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:21<00:00,  1.86it/s]\n",
      "08/11/2022 17:46:00 - INFO - __main__ -   Evaluation done in total 21.482663 secs (0.022854 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:46:00,471 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:46:00,471 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/11/2022 17:46:05 - INFO - __main__ - Results: {'exact': 49.30629669156884, 'f1': 61.34197686639126, 'total': 937, 'HasAns_exact': 49.30629669156884, 'HasAns_f1': 61.34197686639126, 'HasAns_total': 937, 'best_exact': 49.30629669156884, 'best_exact_thresh': 0.0, 'best_f1': 61.34197686639126, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:46:08 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:46:08,652 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:46:08,653 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:46:08,655 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:46:08,655 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:08,656 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:08,656 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:08,656 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:08,656 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:08,656 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:46:08,778 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:46:09,997 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:46:09,998 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:46:15 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/', model_type='scibert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='scibert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:46:15 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/ for evaluation\n",
      "08/11/2022 17:46:15 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:46:15,899 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:46:15,900 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:46:15,900 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:46:16,931 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:46:16,931 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/scibert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:46:17 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_scibert_384_test_bioasq\n",
      "08/11/2022 17:46:17 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:46:17 - INFO - __main__ -   Num examples = 937\n",
      "08/11/2022 17:46:17 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:21<00:00,  1.89it/s]\n",
      "08/11/2022 17:46:38 - INFO - __main__ -   Evaluation done in total 21.208739 secs (0.022635 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:46:38,956 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:46:38,956 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:46:43 - INFO - __main__ - Results: {'exact': 54.749199573105656, 'f1': 68.78859612961512, 'total': 937, 'HasAns_exact': 54.749199573105656, 'HasAns_f1': 68.78859612961512, 'HasAns_total': 937, 'best_exact': 54.749199573105656, 'best_exact_thresh': 0.0, 'best_f1': 68.78859612961512, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:46:46 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:46:46,953 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:46:46,954 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:46:46,955 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:46:46,956 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:46,956 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:46,956 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:46,956 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:46,956 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:46:46,956 >> loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:46:47,037 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:46:48,065 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:46:48,066 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:46:54 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/', model_type='clinicalbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='clinicalbert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:46:54 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/ for evaluation\n",
      "08/11/2022 17:46:54 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:46:54,059 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:46:54,060 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:46:54,060 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:46:55,100 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:46:55,100 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/clinicalbert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:46:55 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_clinicalbert_384_test_bioasq\n",
      "08/11/2022 17:46:55 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:46:55 - INFO - __main__ -   Num examples = 940\n",
      "08/11/2022 17:46:55 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:21<00:00,  1.86it/s]\n",
      "08/11/2022 17:47:17 - INFO - __main__ -   Evaluation done in total 21.509989 secs (0.022883 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:47:17,467 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:47:17,467 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:47:21 - INFO - __main__ - Results: {'exact': 38.10032017075774, 'f1': 55.33346478202084, 'total': 937, 'HasAns_exact': 38.10032017075774, 'HasAns_f1': 55.33346478202084, 'HasAns_total': 937, 'best_exact': 38.10032017075774, 'best_exact_thresh': 0.0, 'best_f1': 55.33346478202084, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:47:25 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:47:25,569 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:47:25,570 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:47:25,571 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:47:25,571 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,571 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,572 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,572 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,572 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,572 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:47:25,572 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:47:25,718 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:47:27,052 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:47:27,052 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:47:33 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/', model_type='roberta', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='roberta', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:47:33 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/ for evaluation\n",
      "08/11/2022 17:47:33 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:47:33,008 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:47:33,009 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:47:33,009 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:47:34,156 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:47:34,156 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/roberta-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:47:34 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_roberta_384_test_bioasq\n",
      "08/11/2022 17:47:35 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:47:35 - INFO - __main__ -   Num examples = 937\n",
      "08/11/2022 17:47:35 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:20<00:00,  1.92it/s]\n",
      "08/11/2022 17:47:55 - INFO - __main__ -   Evaluation done in total 20.836047 secs (0.022237 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:47:55,861 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:47:55,861 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:48:01 - INFO - __main__ - Results: {'exact': 50.907150480256135, 'f1': 64.99118536210071, 'total': 937, 'HasAns_exact': 50.907150480256135, 'HasAns_f1': 64.99118536210071, 'HasAns_total': 937, 'best_exact': 50.907150480256135, 'best_exact_thresh': 0.0, 'best_f1': 64.99118536210071, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:48:04 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:48:04,749 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:48:04,749 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:48:04,751 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:48:04,751 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:04,751 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:04,751 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:04,751 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:04,751 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:04,751 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:48:04,850 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:48:06,070 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:48:06,070 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:48:12 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/', model_type='bluebert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='bluebert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:48:12 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/ for evaluation\n",
      "08/11/2022 17:48:12 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:48:12,165 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:48:12,165 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:48:12,166 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:48:13,210 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:48:13,210 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bluebert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:48:13 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_bluebert_384_test_bioasq\n",
      "08/11/2022 17:48:14 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:48:14 - INFO - __main__ -   Num examples = 938\n",
      "08/11/2022 17:48:14 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:21<00:00,  1.87it/s]\n",
      "08/11/2022 17:48:35 - INFO - __main__ -   Evaluation done in total 21.415476 secs (0.022831 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:48:35,457 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:48:35,457 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/11/2022 17:48:39 - INFO - __main__ - Results: {'exact': 27.2145144076841, 'f1': 40.69504299135788, 'total': 937, 'HasAns_exact': 27.2145144076841, 'HasAns_f1': 40.69504299135788, 'HasAns_total': 937, 'best_exact': 27.2145144076841, 'best_exact_thresh': 0.0, 'best_f1': 40.69504299135788, 'best_f1_thresh': 0.0}\n",
      "08/11/2022 17:48:43 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:48:43,905 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:48:43,905 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:48:43,907 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2022-08-11 17:48:43,907 >> Didn't find file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:43,907 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:43,907 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:43,907 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:43,907 >> loading file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2022-08-11 17:48:43,907 >> loading file None\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:48:44,030 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:48:45,423 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:48:45,423 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:48:51 - INFO - __main__ - Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/net/kdinxidk03/opt/NFS/75y/data/qa/dataset_pos/bioasq/', dataset_name='bioasq', device=device(type='cuda'), do_eval=False, do_evaluate=True, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500000, max_answer_length=200, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='/net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, orig_model_name_or_path='bert', output_dir='/net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/', output_model_dir='None', overwrite_cache=False, overwrite_output_dir=True, overwrite_output_model_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_file='test_bioasq9b.json', save_steps=500000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/11/2022 17:48:51 - INFO - __main__ - Loading checkpoint /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/ for evaluation\n",
      "08/11/2022 17:48:51 - INFO - __main__ - Evaluate the following checkpoints: ['/net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/']\n",
      "[INFO|configuration_utils.py:528] 2022-08-11 17:48:51,329 >> loading configuration file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/config.json\n",
      "[INFO|configuration_utils.py:566] 2022-08-11 17:48:51,329 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2022-08-11 17:48:51,330 >> loading weights file /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2022-08-11 17:48:52,363 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2022-08-11 17:48:52,363 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /net/kdinxidk03/opt/NFS/75y/data/qa/output/bert-bioasq/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "08/11/2022 17:48:52 - INFO - __main__ - Loading features from cached file /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/cached_dev_bert_384_test_bioasq\n",
      "08/11/2022 17:48:53 - INFO - __main__ - ***** Running evaluation bioasq *****\n",
      "08/11/2022 17:48:53 - INFO - __main__ -   Num examples = 938\n",
      "08/11/2022 17:48:53 - INFO - __main__ -   Batch size = 24\n",
      "Evaluating: 100%|███████████████████████████████| 40/40 [00:21<00:00,  1.86it/s]\n",
      "08/11/2022 17:49:14 - INFO - __main__ -   Evaluation done in total 21.467414 secs (0.022886 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2022-08-11 17:49:14,724 >> Writing predictions to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/predictions_bioasq.json\n",
      "[INFO|squad_metrics.py:403] 2022-08-11 17:49:14,724 >> Writing nbest to: /net/kdinxidk03/opt/NFS/75y/data/OTMRC_PAPER/nbest_predictions_bioasq.json\n",
      "08/11/2022 17:49:19 - INFO - __main__ - Results: {'exact': 41.40875133404482, 'f1': 54.094664247792906, 'total': 937, 'HasAns_exact': 41.40875133404482, 'HasAns_f1': 54.094664247792906, 'HasAns_total': 937, 'best_exact': 41.40875133404482, 'best_exact_thresh': 0.0, 'best_f1': 54.094664247792906, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python run_evaluate_source.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
