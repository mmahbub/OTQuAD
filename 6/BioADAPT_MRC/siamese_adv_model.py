import torch
import torch.nn as nn
from transformers import (
    MODEL_FOR_QUESTION_ANSWERING_MAPPING,
    WEIGHTS_NAME,
    AdamW,
    AutoConfig,
    AutoModelForQuestionAnswering,
    get_linear_schedule_with_warmup,
)
from BioADAPT_MRC.enc_disc_qa import (Encoder,
                         Disc_QA_Out,
                         QA_Out,
                         ReverseLayerF
                         )
import BioADAPT_MRC.configs as configs


class siamese_adv_net(nn.Module):  # ekhane shob call hobe
    def __init__(self):
        super(siamese_adv_net, self).__init__()

        configs.model_type = configs.model_type.lower()
        config = AutoConfig.from_pretrained(
            configs.config_name if configs.config_name else configs.pretrained_model_name_or_path,
            cache_dir=configs.cache_dir if configs.cache_dir else None,
        )

        pretrained_model = AutoModelForQuestionAnswering.from_pretrained(
            configs.pretrained_model_name_or_path,
            from_tf=bool(".ckpt" in configs.pretrained_model_name_or_path),
            config=config,
            cache_dir=configs.cache_dir if configs.cache_dir else None,
        )

        self.encoder = Encoder(pretrained_model,
                               freeze_encoder=configs.freeze_encoder)

        self.factoid_qa_output_generator = QA_Out(pretrained_model,
                                                  freeze_qa_output_generator=configs.freeze_qa_output_generator)

        ## following layers need to be trained from scratch
        self.disc_aux_qa_layer = Disc_QA_Out(freeze_aux_qa_output_generator=configs.freeze_aux_qa_output_generator, 
                                             freeze_discriminator_encoder=configs.freeze_discriminator_encoder)

        self.disc_aux_qa_layer.apply(self.init_weights)

    def forward(self, data_batch):
        '''
        question_context_features ->
                ([input_ids_0, attention_mask_0, token_type_ids_0, start_positions_0, end_positions_0], 
                 [input_ids_1, attention_mask_1, token_type_ids_1, start_positions_1, end_positions_1]) (generated by the dataset generator online)
            (reference: https://github.com/Sazan-Mahbub/EGRET/blob/main/EGRET/feature_generator/ProtBERT_feature_generator.py#L80
        
        '''
        question_context_features, start_positions, end_positions = self.move_to_cuda(data_batch)

        encodings = self.siamese_encoder(question_context_features)
        factoid_qa_outputs, original_qa_loss = self.siamese_factoid_qa_out_gen(encodings,
                                                                               start_positions,
                                                                               end_positions)
        adv_loss = None
        aux_qa_loss = None
        aux_qa_outputs = None
        total_loss = configs.qa_loss_alpha * original_qa_loss
        if configs.do_train and configs.domain_adaptation:
            adv_loss, aux_qa_outputs, aux_qa_loss, cosDist_source, cosDist_target = self.siamese_discriminator(encodings,
                                                                               start_positions,
                                                                               end_positions)
            total_loss += configs.adv_loss_beta * adv_loss + \
                          configs.aux_layer_gamma * aux_qa_loss
            
        return encodings, factoid_qa_outputs, aux_qa_outputs, adv_loss, aux_qa_loss, original_qa_loss, total_loss, cosDist_source, cosDist_target

    def init_weights(self, module):
        """Initialize the weights"""
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=configs.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=configs.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def move_to_cuda(self, data_batch):
        question_context_features, start_positions, end_positions = data_batch
        for key in question_context_features[0]:
            question_context_features[0][key] = torch.autograd.Variable(
                question_context_features[0][key].to(configs.device).long())
            question_context_features[1][key] = torch.autograd.Variable(
                question_context_features[1][key].to(configs.device).long())
            question_context_features[2][key] = torch.autograd.Variable(
                question_context_features[2][key].to(configs.device).long())

        start_positions[0] = torch.autograd.Variable(start_positions[0].to(configs.device).long())
        end_positions[0] = torch.autograd.Variable(end_positions[0].to(configs.device).long())
        start_positions[1] = torch.autograd.Variable(start_positions[1].to(configs.device).long())
        end_positions[1] = torch.autograd.Variable(end_positions[1].to(configs.device).long())
        start_positions[2] = torch.autograd.Variable(start_positions[2].to(configs.device).long())
        end_positions[2] = torch.autograd.Variable(end_positions[2].to(configs.device).long())
        return question_context_features, start_positions, end_positions

    def siamese_encoder(self, question_context_features):  # how about 2 batches from 2 domains
        # encoding of samples from each domain 
        # (there will be two sets for two domains, represented by the question_contex_feature)
        encoding_domain_0 = self.encoder(question_context_features[0])  # out dim: B X 512 X 784 (making sure that it comes from the very last layer of electra)
        encoding_domain_1 = self.encoder(question_context_features[1])  # out dim: B X 512 X 784
        encoding_domain_2 = self.encoder(question_context_features[2])  # out dim: B X 512 X 784
        return [encoding_domain_0, encoding_domain_1, encoding_domain_2]

    def siamese_discriminator(self, encodings, start_positions, end_positions):
        '''
            generate the probability of the two samples coming from the same domain 
            :param encodings:
            :return:
        '''
        # todo: implement encoder forward pass 
        encoding_domain_0, encoding_domain_1, encoding_domain_2 = encodings

        reversed_representation_0 = self.gradient_reversal_layer(encoding_domain_0)  # out dim: B X 512 X 784 
        reversed_representation_0, aux_qa_outputs_0 = self.disc_aux_qa_layer(reversed_representation_0,
                                                                             start_positions[0],
                                                                             end_positions[0])  # out dim: B X 1 X dim_domain_rep

        reversed_representation_1 = self.gradient_reversal_layer(encoding_domain_1)  # out dim: B X 512 X 784 
        reversed_representation_1, aux_qa_outputs_1 = self.disc_aux_qa_layer(reversed_representation_1,
                                                                             start_positions[1],
                                                                             end_positions[1])  # out dim: B X 1 X dim_domain_rep

        reversed_representation_2 = self.gradient_reversal_layer(encoding_domain_2)  # out dim: B X 512 X 784
        reversed_representation_2, aux_qa_outputs_2 = self.disc_aux_qa_layer(reversed_representation_2,
                                                                             start_positions[2],
                                                                             end_positions[2])  # out dim: B X 1 X dim_domain_rep
        
        triplet_loss = torch.nn.TripletMarginWithDistanceLoss(
            distance_function=lambda x, y: 1.0 - torch.nn.functional.cosine_similarity(x, y),
            margin=2.0, swap=True, reduction='none')

        adv_loss = triplet_loss(anchor=reversed_representation_0,
                                positive=reversed_representation_1,
                                negative=reversed_representation_2)[0].cuda()
        
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        cosDist_source = 1.0 - torch.nn.functional.cosine_similarity(reversed_representation_0,
                                                              reversed_representation_1)[0].cpu().detach().numpy()
        cosDist_target = 1.0 - torch.nn.functional.cosine_similarity(reversed_representation_0,
                                                              reversed_representation_2)[0].cpu().detach().numpy()
        
#         print(cosDist_source, cosDist_target)
#         print(adv_loss, 1-cosine_similarity(reversed_representation_0.cpu().detach().numpy(),
#                                           reversed_representation_1.cpu().detach().numpy()),
#                         1-cosine_similarity(reversed_representation_0.cpu().detach().numpy(),
#                                           reversed_representation_2.cpu().detach().numpy()))

        
        aux_qa_loss = aux_qa_outputs_0.loss
        if configs.do_train:
            aux_qa_loss += configs.aux_layer_domain_0_gamma * aux_qa_outputs_1.loss
            if configs._factoid_qa_generate_both_:  # target label consider korle True hobe
                aux_qa_loss += configs.aux_layer_domain_1_gamma * aux_qa_outputs_2.loss
                aux_qa_loss /= 3.

        return adv_loss, \
               [aux_qa_outputs_0, aux_qa_outputs_1, aux_qa_outputs_2], \
               aux_qa_loss, cosDist_source, cosDist_target  # probability that the domains of the two inputs are the same

    def gradient_reversal_layer(self, encoding):
        '''
            generate an output with the same floating values but during backpass their gradient will be negated
        '''
        reversed_feature = ReverseLayerF.apply(encoding.last_hidden_state,
                                               configs.reverse_layer_lambda)  #ref: https://github.com/SongweiGe/scDGN/blob/master/utils/model_util.py
        return reversed_feature

    def siamese_factoid_qa_out_gen(self, encodings, start_positions, end_positions):
        '''
           siamese_factoid_qa_out_gen. 
           Two possible cases: 
                (1) generate the factoid_qa_output for only domain_0: Previous methods did this. I donnow why this was the case, and why not taking both.
                (2) generate the factoid_qa_output for both domain_0 and domain_1: I donnow if this will work better or not.
        '''
        encoding_domain_0, encoding_domain_1, encoding_domain_2 = encodings

        # _factoid_qa_generate_both_ = True # do not take this value like this. take it from a config file.

        factoid_qa_output_0 = self.factoid_qa_output_generator(encoding_domain_0,
                                                               start_positions[0],
                                                               end_positions[0])  # some codes to manipulate encoding_domain_0....
        total_loss = factoid_qa_output_0.loss
        factoid_qa_output_1, factoid_qa_output_2 = None, None
        if configs.do_train:
            factoid_qa_output_1 = self.factoid_qa_output_generator(encoding_domain_1,
                                                                   start_positions[1],
                                                                   end_positions[1])  # some codes to manipulate encoding_domain_1....
            total_loss += configs.qa_loss_domain_0_alpha*factoid_qa_output_1.loss
            if configs._factoid_qa_generate_both_:  # default True hobe, both bolte "both domains" bujhay
                factoid_qa_output_2 = self.factoid_qa_output_generator(encoding_domain_2,
                                                                       start_positions[2],
                                                                       end_positions[2])  # some codes to manipulate encoding_domain_1....
                total_loss += configs.qa_loss_domain_1_alpha*factoid_qa_output_2.loss
                total_loss /= 3.
                
        return [factoid_qa_output_0, factoid_qa_output_1, factoid_qa_output_2], total_loss